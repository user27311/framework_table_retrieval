{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "dataset = pt.get_dataset('irds:cord19/fulltext/trec-covid')\n",
    "from collections import Counter\n",
    "import krippendorff\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maj_vote(nums):\n",
    "    if not nums:\n",
    "        return None  # Return None if the list is empty\n",
    "    \n",
    "    count = Counter(sorted(nums, reverse=True))  # Count the frequency of each number\n",
    "    most_common = count.most_common(1)[0][0]  # Get the number with the highest frequency\n",
    "    return most_common\n",
    "\n",
    "def avg_vote(nums):\n",
    "    if not nums:\n",
    "        return 0  # Return 0 if the list is empty\n",
    "    total_sum = sum(nums)  # Calculate the total sum of the list\n",
    "    average = total_sum / len(nums)  # Calculate the average\n",
    "    return round(average, 2)\n",
    "\n",
    "def extract_labels(row, only_human=True):\n",
    "    if only_human:\n",
    "        print(row.index[row.index.str.contains('label_rater')])\n",
    "        row = row[row.index.str.contains('label_rater')]\n",
    "    else:\n",
    "        row = row[row.index.str.contains('label')]\n",
    "    result = []\n",
    "    for entry in row:\n",
    "        result.append(entry)\n",
    "\n",
    "    return result\n",
    "\n",
    "def calc_cohens_kappa(df, names, vis = True):\n",
    "    rater1_labels = df[names[0]]\n",
    "    rater2_labels = df[names[1]]\n",
    "\n",
    "    # Compute Cohen's Weighted Kappa with quadratic weights\n",
    "    kappa = cohen_kappa_score(rater1_labels, rater2_labels, weights='quadratic')\n",
    "\n",
    "    if vis:\n",
    "        conf_matrix = confusion_matrix(rater1_labels, rater2_labels, labels=[0, 1, 2])\n",
    "\n",
    "        # Create a heatmap for visualization\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=[0, 1, 2], yticklabels=[0, 1, 2])\n",
    "        plt.xlabel(f'{names[1]} Labels')\n",
    "        plt.ylabel(f'{names[0]} Labels')\n",
    "        plt.title(f'Confusion Matrix between {names[0]} and {names[1]}')\n",
    "        plt.show()\n",
    "\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_r1 = pd.read_json(\"/workspaces/CORD19_Plus/data/ratings_r1.json\")\n",
    "ratings_r2 = pd.read_json(\"/workspaces/CORD19_Plus/data/ratings_r2.json\")\n",
    "ratings_r3 = pd.read_json(\"/workspaces/CORD19_Plus/data/ratings_r3.json\")\n",
    "ratings_r4 = pd.read_json(\"/workspaces/CORD19_Plus/data/ratings_r4.json\")\n",
    "ratings_r5 = pd.read_json(\"/workspaces/CORD19_Plus/data/ratings_r5.json\")\n",
    "\n",
    "\n",
    "rating_gpt5 = pd.read_json(\"/workspaces/CORD19_Plus/data/labeling/table_pool_qrels5.json\")\n",
    "\n",
    "rating_surrogate = pd.read_json(\"/workspaces/CORD19_Plus/data/ratings_surrogate.json\")\n",
    "\n",
    "#set missing values to 0\n",
    "rating_surrogate['label_surrogate'] = rating_surrogate['label_surrogate'].apply(lambda x: 0 if x == -1 else x)\n",
    "\n",
    "df1_renamed = ratings_r1.rename(columns={'label': 'label_rater_1','parsing' : 'pars_1'})\n",
    "df2_renamed = ratings_r2.rename(columns={'label': 'label_rater_2', 'parsing' : 'pars_2'})\n",
    "df3_renamed = ratings_r3.rename(columns={'label': 'label_rater_3', 'parsing' : 'pars_3'})\n",
    "df4_renamed = ratings_r4.rename(columns={'label': 'label_rater_4', 'parsing' : 'pars_4'})\n",
    "df5_renamed = ratings_r5.rename(columns={'label': 'label_rater_5', 'parsing' : 'pars_5'})\n",
    "\n",
    "df6_renamed = rating_surrogate.rename(columns={'label': 'label_surrogate'})\n",
    "\n",
    "\n",
    "df7_renamed = rating_gpt5.rename(columns={'label': 'label_gpt5'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfs = [df1_renamed, df2_renamed, df3_renamed, df4_renamed, df5_renamed, df6_renamed, df7_renamed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also include a surrogate rater where the judgment origins from the trec-covid dataset.\n",
    "#label_map = {}\n",
    "#surrogate_ratings = dataset.get_qrels()\n",
    "#for _, row in surrogate_ratings.iterrows():\n",
    "#    label_map[(row['qid'], row['docno'])] = row['label']\n",
    "\n",
    "#dummy_df['label_surrogate'] = dummy_df.apply(lambda row: label_map[(str(row['qid']), row['docno'].split(\"_\")[0])] if (str(row['qid']), row['docno'].split(\"_\")[0]) in label_map.keys() else -1, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    df1_renamed,\n",
    "    df2_renamed, \n",
    "    on=['qid', 'docno'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "for df in dfs[2:]:\n",
    "    merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    df,\n",
    "    on=['qid', 'docno'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['maj_vote'] = merged_df.apply(lambda row: maj_vote(extract_labels(row)), axis = 1)\n",
    "merged_df['avg_vote'] = merged_df.apply(lambda row: avg_vote(extract_labels(row)), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rater_columns = [col for col in merged_df.columns if col.startswith('label_rater')]\n",
    "ratings = merged_df[rater_columns]\n",
    "\n",
    "# Transpose the DataFrame to match krippendorff's expected input\n",
    "# Each row represents a rater, each column represents an item\n",
    "reliability_data = ratings.to_numpy().T\n",
    "\n",
    "# Compute Krippendorff's Alpha for ordinal data\n",
    "alpha = krippendorff.alpha(reliability_data, level_of_measurement='ordinal')\n",
    "\n",
    "print(f\"Krippendorff's Alpha (ordinal): {alpha:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = ['maj_vote']\n",
    "all_names += [name for name in merged_df.columns if 'label' in name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_pairs = []\n",
    "\n",
    "for i in range(len(all_names)):\n",
    "    for j in range(i + 1, len(all_names)):\n",
    "        possible_pairs.append((all_names[i], all_names[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in possible_pairs:\n",
    "    if \"maj\" in pair[0]:\n",
    "        print(f\"Cohen's Weighted Kappa for {str(pair):<45} : {calc_cohens_kappa(merged_df, pair, vis=False):>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing_cols = [col for col in merged_df.columns if col.startswith('pars')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[:,parsing_cols].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[:,parsing_cols].mean().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rater_prefix = 'label_rater'\n",
    "gpt_prefix = 'label_gpt'\n",
    "\n",
    "# Collect all labeler columns\n",
    "labeler_cols = [col for col in merged_df.columns if col.startswith(rater_prefix)]\n",
    "\n",
    "print(\"Labeler Columns:\", labeler_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = merged_df.groupby('qid')[labeler_cols].sum().reset_index()\n",
    "\n",
    "print(aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_aggregated_df = aggregated_df.melt(id_vars='qid', var_name='labeler', value_name='sum_labels')\n",
    "\n",
    "print(melted_aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = melted_aggregated_df.pivot(index='qid', columns='labeler', values='sum_labels').fillna(0).reset_index()\n",
    "\n",
    "print(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
