{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from multiprocessing import Manager\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "manager = Manager()\n",
    "\n",
    "db_vals = dotenv_values(\"/workspaces/CORD19_Plus/.env\")\n",
    "from cord19_plus.data_model.model import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset('irds:cord19/fulltext/trec-covid')\n",
    "topics = dataset.get_topics()\n",
    "qrels = dataset.get_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tables to label\n",
    "engine = create_engine(f\"postgresql+psycopg2://{db_vals['USER']}:{db_vals['PASSWORD']}@{db_vals['ADDRESS']}:{db_vals['PORT']}/{db_vals['DB_FINAL']}\", echo=False)\n",
    "session = Session(engine)\n",
    "\n",
    "result = session.query(Table)\n",
    "result_dict = [{\"docno\" : str(e.ir_tab_id),\n",
    "                \"ir_id\" : e.ir_id, \n",
    "                \"pm_content\" : e.pm_content, \n",
    "                \"header\" : e.header, \n",
    "                \"content\" : e.content,\n",
    "                \"caption\" : e.caption,\n",
    "                \"references\" : e.references,\n",
    "               } for e in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [res['docno'] for res in result_dict]\n",
    "len(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_paths = sorted(glob.glob(\"/workspaces/CORD19_Plus/data/prompts/*\"), key=lambda x: int(x.split(\"_\")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "\n",
    "for path in prompt_paths:\n",
    "    with open(path, 'r') as file:\n",
    "        prompts.append(file.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temperature = 0 to suppress creativity\n",
    "\n",
    "prompt_version = 5\n",
    "prompt_t = prompts[prompt_version-1]\n",
    "\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "#model_name = \"gpt-4o\"\n",
    "\n",
    "prompt= PromptTemplate(template=prompt_t, input_variables=[\"QUERY\", \n",
    "                                                                  \"QUESTION\",\n",
    "                                                                  \"NARRATIVE\",\n",
    "                                                                  \"CONTENT\",\n",
    "                                                                  \"CAPTION\",\n",
    "                                                                  \"REFERENCES\"])\n",
    "model = ChatOpenAI(temperature=0, model=model_name)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "#chain = prompt | model \n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "manager = Manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'topics' is a pandas DataFrame with 'qid', 'title', 'description', and 'narrative' columns\n",
    "# 'result_dict' is a list of dictionaries with keys like 'docno', 'content', 'header', 'caption', 'references'\n",
    "# 'pool' is a pandas DataFrame with 'qid' and 'docno' columns\n",
    "# 'chain' is an object with an 'invoke' method that processes input_data\n",
    "\n",
    "def handle_request(topic, table, shared_qrels, lock):\n",
    "    \"\"\"\n",
    "    Processes a single document within a topic and updates the shared_qrels dictionary.\n",
    "\n",
    "    Args:\n",
    "        topic (pd.Series): A row from the topics DataFrame representing a single topic.\n",
    "        table (dict): A dictionary containing document information.\n",
    "        shared_qrels (dict): A shared dictionary to store results.\n",
    "        lock (threading.Lock): A lock to ensure thread-safe updates to shared_qrels.\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        \"QUERY\": topic['title'],\n",
    "        \"QUESTION\": topic['description'],\n",
    "        \"NARRATIVE\": topic['narrative'],\n",
    "        \"CONTENT\": tabulate(table['content'], headers=table['header'], tablefmt=\"github\"),\n",
    "        \"CAPTION\": table['caption'],\n",
    "        \"REFERENCES\": \" | \".join(table['references'])\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Invoke the chain's method to process the input_data\n",
    "        res = chain.invoke(input_data)\n",
    "\n",
    "        # Safely update the shared_qrels dictionary\n",
    "        with lock:\n",
    "            shared_qrels[topic['qid']].append((table['docno'], int(res)))\n",
    "    except Exception as e:\n",
    "        # Handle exceptions gracefully, optionally logging them\n",
    "        print(f\"Error processing docno {table['docno']} in topic {topic['qid']}: {e}\")\n",
    "\n",
    "def process_topic(topic, result_dict, shared_qrels, pool, lock, results_dir):\n",
    "    \"\"\"\n",
    "    Processes all documents within a single topic using a ThreadPoolExecutor and saves the results.\n",
    "\n",
    "    Args:\n",
    "        topic (pd.Series): A row from the topics DataFrame representing a single topic.\n",
    "        result_dict (list): A list of document dictionaries to process.\n",
    "        shared_qrels (dict): A shared dictionary to store results.\n",
    "        pool (pd.DataFrame): A DataFrame containing 'qid' and 'docno' columns for filtering.\n",
    "        lock (threading.Lock): A lock to ensure thread-safe updates to shared_qrels.\n",
    "        results_dir (str): Directory path where results are saved.\n",
    "    \"\"\"\n",
    "    qid = topic['qid']\n",
    "    result_file = os.path.join(results_dir, f\"qid_{qid}.json\")\n",
    "\n",
    "    # Check if the result file already exists to avoid reprocessing\n",
    "    if os.path.exists(result_file):\n",
    "        print(f\"Results for qid {qid} already exist. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    # Initialize the list for this topic\n",
    "    with lock:\n",
    "        shared_qrels[qid] = []\n",
    "\n",
    "    # Filter documents based on the pool if provided\n",
    "    if not pool.empty:\n",
    "        pool_topic_ids = pool[pool['qid'] == int(qid)]['docno'].to_list()\n",
    "        tables_to_process = [table for table in result_dict if table['docno'] in pool_topic_ids]\n",
    "    else:\n",
    "        tables_to_process = result_dict\n",
    "\n",
    "    if not tables_to_process:\n",
    "        # No documents to process for this topic\n",
    "        print(f\"No documents to process for qid {qid}.\")\n",
    "        return\n",
    "\n",
    "    # Use a ThreadPoolExecutor to process documents in parallel\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  # Adjust max_workers as needed\n",
    "        futures = [\n",
    "            executor.submit(handle_request, topic, table, shared_qrels, lock)\n",
    "            for table in tables_to_process\n",
    "        ]\n",
    "\n",
    "        # Ensure all document processing tasks complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # Retrieve result to catch exceptions\n",
    "            except Exception as e:\n",
    "                # Exceptions are already handled in handle_request; this is optional\n",
    "                pass\n",
    "\n",
    "    # After processing all documents for the topic, save the results to a file\n",
    "    try:\n",
    "        with lock:\n",
    "            # Prepare data to save\n",
    "            data_to_save = shared_qrels[qid]\n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Results for qid {qid} saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results for qid {qid}: {e}\")\n",
    "\n",
    "def load_existing_results(results_dir):\n",
    "    \"\"\"\n",
    "    Loads existing results from the results directory into a dictionary.\n",
    "\n",
    "    Args:\n",
    "        results_dir (str): Directory path where results are saved.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping qid to a list of (docno, score) tuples.\n",
    "    \"\"\"\n",
    "    shared_qrels = {}\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "        return shared_qrels\n",
    "\n",
    "    for filename in os.listdir(results_dir):\n",
    "        if filename.startswith(\"qid_\") and filename.endswith(\".json\"):\n",
    "            qid_str = filename[4:-5]  # Extract qid from filename\n",
    "            try:\n",
    "                qid = int(qid_str)\n",
    "                file_path = os.path.join(results_dir, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                shared_qrels[qid] = [tuple(item) for item in data]\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    return shared_qrels\n",
    "\n",
    "def label_full_pool(pool, topics, result_dict, results_dir):\n",
    "    \"\"\"\n",
    "    Processes all topics and their associated documents, tracking overall progress with a tqdm bar.\n",
    "    Saves results after each topic is processed and allows resuming from interruptions.\n",
    "\n",
    "    Args:\n",
    "        pool (pd.DataFrame): A DataFrame containing 'qid' and 'docno' columns for filtering.\n",
    "        topics (pd.DataFrame): A DataFrame containing topic information with 'qid', 'title', 'description', and 'narrative'.\n",
    "        result_dict (list): A list of document dictionaries to process.\n",
    "        results_dir (str): Directory path where results are saved. Defaults to 'results'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each qid to a list of (docno, score) tuples.\n",
    "    \"\"\"\n",
    "    shared_qrels = load_existing_results(results_dir)\n",
    "    lock = Lock()  # To ensure thread-safe operations on shared_qrels\n",
    "\n",
    "    num_topics = len(topics)\n",
    "    processed_topics = len(shared_qrels)\n",
    "    remaining_topics = num_topics - processed_topics\n",
    "\n",
    "    print(f\"Total topics: {num_topics}\")\n",
    "    print(f\"Already processed topics: {processed_topics}\")\n",
    "    print(f\"Remaining topics to process: {remaining_topics}\")\n",
    "\n",
    "    # Initialize the overall progress bar for topics\n",
    "    with tqdm(total=num_topics, desc=\"Overall Progress\") as overall_pbar:\n",
    "        # Update the progress bar for already processed topics\n",
    "        overall_pbar.update(processed_topics)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust max_workers based on your system\n",
    "            # Submit all topic processing tasks\n",
    "            futures = [\n",
    "                executor.submit(\n",
    "                    process_topic,\n",
    "                    topic,\n",
    "                    result_dict,\n",
    "                    shared_qrels,\n",
    "                    pool,\n",
    "                    lock,\n",
    "                    results_dir\n",
    "                )\n",
    "                for _, topic in topics.iterrows()\n",
    "            ]\n",
    "\n",
    "            # Iterate over the completed futures to update the progress bar\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    future.result()  # Retrieve result to catch exceptions\n",
    "                except Exception as e:\n",
    "                    # Handle exceptions gracefully, optionally logging them\n",
    "                    print(f\"Error processing topic: {e}\")\n",
    "                finally:\n",
    "                    overall_pbar.update(1)  # Update the overall progress bar after each topic\n",
    "\n",
    "    return shared_qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = pd.read_json(\"/workspaces/CORD19_Plus/retrieval/pool.json\")\n",
    "pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_pool = pool[pool['rank'] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = pd.read_json(\"/workspaces/CORD19_Plus/retrieval/pool.json\")\n",
    "pool_qrels = label_full_pool(pool, topics, result_dict=result_dict, results_dir=f'/workspaces/CORD19_Plus/data/labeling/labeling_results{prompt_version}-final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for qid, docs in pool_qrels.items():\n",
    "    for docno, label in docs:\n",
    "        rows.append({'qid': qid, 'docno': docno, 'label': label})\n",
    "\n",
    "table_qrels = pd.DataFrame(rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_path = f\"/workspaces/CORD19_Plus/data/labeling/table_pool_qrels{prompt_version}-final.json\"\n",
    "table_qrels.to_json(qrels_path, orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
