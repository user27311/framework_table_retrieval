{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "if not pt.started():\n",
    "   pt.init()\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset('irds:cord19/fulltext/trec-covid')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qrels = dataset.get_qrels()\n",
    "rel_doc_nos = qrels['docno'].unique().tolist()\n",
    "len(rel_doc_nos)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#metadata = pd.read_csv(\"/workspaces/CORD19_Plus/data/metadata.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#metadata_filtered = metadata[metadata['cord_uid'].isin(rel_doc_nos)]\n",
    "#save filtered metadata to a csv file\n",
    "#metadata_filtered.to_csv('/workspaces/CORD19_Plus/data/rel_metadata.csv', index=False)\n",
    "metadata_filtered = pd.read_csv('/workspaces/CORD19_Plus/data/rel_metadata.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get current publications\n",
    "\n",
    "def load_jsonl_to_dataframe(file_path):\n",
    "    # Initialize an empty list to collect JSON objects\n",
    "    data_list = []\n",
    "    \n",
    "    # Open the JSON Lines file and read line by line\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Parse each line as JSON and append it to the list\n",
    "            data_list.append(json.loads(line.strip()))\n",
    "    \n",
    "    # Convert the list of JSON objects into a DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_pdf_url(res):\n",
    "    res_dict = res.json()\n",
    "\n",
    "    for _, val in res_dict.items():\n",
    "        if isinstance(val, dict):\n",
    "            for k,v in val.items():\n",
    "                if k == \"url_for_pdf\":\n",
    "                    return v\n",
    "    return None\n",
    "\n",
    "def build_api_url_call(doi, email=\"unpaywall_01@example.com\"):\n",
    "    return f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
    "\n",
    "\n",
    "# Function to handle each request and extract the PDF URL\n",
    "def fetch_pdf_url(missing_doi):\n",
    "    res = requests.get(build_api_url_call(missing_doi))\n",
    "    pdf_url = extract_pdf_url(res)\n",
    "    if pdf_url:\n",
    "        return missing_doi, pdf_url\n",
    "    return missing_doi, None\n",
    "\n",
    "def append_to_jsonl(file_path, data):\n",
    "    \"\"\"\n",
    "    Appends a dictionary as a new line to a JSONL file.\n",
    "\n",
    "    :param file_path: The path to the JSONL file.\n",
    "    :param data: A dictionary representing the row you want to append.\n",
    "                 It should be in the same format as the existing JSONL entries.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        # Convert the dictionary to a JSON string and append it to the file\n",
    "        file.write(json.dumps(data) + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = load_jsonl_to_dataframe(\"/workspaces/CORD19_Plus/data/index.jsonl\")\n",
    "avail_ids = df['key'].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(avail_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(set(rel_doc_nos))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "14704/37924"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_ids = set(rel_doc_nos).difference(set(avail_ids))\n",
    "len(missing_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_dois = metadata_filtered[metadata_filtered['cord_uid'].isin(missing_ids)]['doi'].unique().tolist()\n",
    "doi2cord_uid_map = {} \n",
    "len(missing_dois)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for missing_doi in tqdm(missing_dois):\n",
    "    try:\n",
    "        doi2cord_uid_map[missing_doi] = metadata_filtered[metadata_filtered['doi'] == missing_doi]['cord_uid'].iloc[0]\n",
    "    except:\n",
    "        print(missing_doi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdf_urls = {}\n",
    "max_workers = 25\n",
    "\n",
    "# Use ThreadPoolExecutor to run the requests in parallel\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Create a dictionary to store the future object and the corresponding missing_id\n",
    "    future_to_id = {executor.submit(fetch_pdf_url, missing_doi): missing_doi for missing_doi in missing_dois}\n",
    "    \n",
    "    # Iterate over completed futures and update the progress bar\n",
    "    for future in tqdm(as_completed(future_to_id), total=len(missing_dois)):\n",
    "        missing_doi, pdf_url = future.result()\n",
    "        if pdf_url:\n",
    "            pdf_urls[missing_doi] = pdf_url\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pickle.dump(pdf_urls, open(\"/workspaces/CORD19_Plus/data/next_pdf_urls.pkl\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "urls = pickle.load(open(\"/workspaces/CORD19_Plus/data/next_pdf_urls.pkl\", \"rb\"))\n",
    "len(urls)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_mising_pdf(doi, url, doi2cord_uid_map, path_to_save = \"/workspaces/CORD19_Plus/data/pdfs\"):\n",
    "    filename = path_to_save + \"/\" + doi2cord_uid_map[doi] + \".pdf\"\n",
    "    try:\n",
    "        if not os.path.exists(filename):\n",
    "            response = requests.get(url)\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "    except:\n",
    "        print(f\"{doi2cord_uid_map[doi]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#now download those missing pdfs and update index\n",
    "import os\n",
    "\n",
    "max_workers = 15\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Create a dictionary to store the future object and the corresponding missing_id\n",
    "    future_to_id = {executor.submit(download_mising_pdf, doi, url, doi2cord_uid_map): doi for doi, url in urls.items()}\n",
    "    \n",
    "    # Iterate over completed futures and update the progress bar\n",
    "    for future in tqdm(as_completed(future_to_id), total=len(urls)):\n",
    "        _ = future.result()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with tqdm(total = len(urls)) as pbar:\n",
    "    for key, val in urls.items():\n",
    "        data = {}\n",
    "        data['status'] = 1  \n",
    "        data['key'] = doi2cord_uid_map[key]\n",
    "        data['pdf_path'] = f\"{data['key']}.pdf\"\n",
    "        data['pdf_url'] = val\n",
    "\n",
    "        append_to_jsonl(\"/workspaces/CORD19_Plus/data/index.jsonl\",data)\n",
    "        pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
